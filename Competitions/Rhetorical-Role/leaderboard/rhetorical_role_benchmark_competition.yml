# Allows at most 5 submissions per user per period, where period is 24 hours by default.
username: 'codalab username'
password: 'codalab password'
max_submissions_per_period: 5

# UUID of the worksheet where prediction and evaluation bundles are created for submissions.
log_worksheet_uuid: 'sample uuid'

# Configure the tag that participants use to submit to the competition.
# In this example, any bundle with the tag `some-competition-submit` would be
# considered as an official submission.
submission_tag: your_submission_tag

# Configure how to mimic the submitted prediction bundles. When evaluating a submission, 
# `new` bundle will replace `old` bundle.
# For a machine learning competition, `old` bundle might be the dev set and `new` bundle
# might be the hidden test set.
predict:
  mimic:
  - {new: 'sample uuid', old: 'sample uuid'}

# Configure how to evaluate the new prediction bundles.
# In this example, evaluate.py is script that takes in the paths of the test labels and 
# predicted labels and outputs the evaluation results.
evaluate:
  # Essentially
  #     cl run evaluate.py:0x089063eb85b64b239b342405b5ebab57 \
  #            test.json:0x5538cba32e524fad8b005cd19abb9f95 \
  #            predictions.json:{predict}/predictions.json --- \
  #            python evaluate.py test.json predictions.json
  # where {predict} gets filled in with the uuid of the mimicked bundle above.
  dependencies:
  - {child_path: evaluation.py, parent_uuid: 'sample uuid'}
  - {child_path: TEST_GROUND_TRUTH.json, parent_uuid: 'sample uuid'}
  - {child_path: predictions.json, parent_path: predictions.json, parent_uuid: '{predict}'}
  command: python evaluation.py TEST_GROUND_TRUTH.json predictions.json

# Define how to extract the scores from the evaluation bundle.
# In this example, result.json is a JSON file outputted from the evaluation step
# with F1 and exact match metrics (e.g. {"f1": 91, "exact_match": 92}).
score_specs:
- {key: '/result.json:Weighted-F1', name: Weighted-F1}